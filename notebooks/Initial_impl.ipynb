{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "context = \\\"\\\"\\\"RECURSIVE LANGUAGE MODELS\\n\\nAlex L. Zhang, Tim Kraska, Omar Khattab\\nMIT CSAIL\\n\\nABSTRACT\\n\\nWe study allowing large language models (LLMs) to process arbitrarily long\\nprompts through the lens of inference-time scaling. We propose Recursive Language\\nModels (RLMs), a general inference strategy that treats long prompts as\\npart of an external environment and allows the LLM to programmatically examine,\\ndecompose, and recursively call itself over snippets of the prompt.\\n\\n1 INTRODUCTION\\n\\nDespite rapid progress in reasoning and tool use, modern language models still have limited context\\nlengths and, even within these limits, appear to inevitably exhibit context rot.\\n\\n2 SCALING LONG CONTEXT TASKS\\n\\nRecent work has successfully argued that the effective context window of LLMs can often be much\\nshorter than a model's physical maximum number of tokens.\\n\\\"\\\"\\\"\\n\\ncontext_type = \\\"str\\\"\\ncontext_total_length = len(context)\\ncontext_lengths = [context_total_length]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}